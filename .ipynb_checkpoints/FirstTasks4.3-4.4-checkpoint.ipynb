{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e37f66ef-52e1-4d64-836e-538f5608e33b",
   "metadata": {},
   "source": [
    "# PROYECTO REFFINDR\n",
    "### Seguimiento de Tareas de José María Pacheco Benítez en su rol de Data Analist para el proyecto REFFINDR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a9d9ee-f894-4537-a2d6-7cacbe5b9054",
   "metadata": {},
   "source": [
    "Voy a trabajar con Python para implementar el ETL, y necesitaré conectar con una API de datos inmobiliarios y cargar los datos en PostgreSQL.\n",
    "\n",
    "## Paso 1: Configura el entorno\n",
    "Asegúrate de tener instaladas las bibliotecas necesarias en tu entorno. Si no lo has hecho, activa tu entorno conda para el proyecto y ejecuta:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1054a0d-0880-45a4-8011-b97ac3d27971",
   "metadata": {},
   "source": [
    "En mi entorno de trabajo he creado con Conda el siguiente environment:\n",
    "\r\n",
    "conda create -n reffindr python=3.9\r\n",
    "# Bibliotecas para manipulación y análisis de datos\r\n",
    "conda install pandas numpy sqlalchemy\r\n",
    "\r\n",
    "# Biblioteca para ETL y conexión con APIs\r\n",
    "conda install requests\r\n",
    "\r\n",
    "# Bibliotecas para visualización\r\n",
    "conda install matplotlib seaborn\r\n",
    "\r\n",
    "# Para conectar y trabajar con conda install -c conda-forge psycopg2 -c conda-fo\n",
    "\n",
    "# Para poder lanzar jupyter lab desde el en\n",
    "orno creadorconda insrge psycopg2\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e26ee-9ddf-4a34-aa48-73f8cd4378ad",
   "metadata": {},
   "source": [
    "## Paso 1.1 Para el trabajo y seguimiento en jupyter lab, seguir con las siguientes celdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c9f06c5-d520-4d32-9b60-d965f1031120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\usuario\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\usuario\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\usuario\\anaconda3\\lib\\site-packages (2.0.25)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from sqlalchemy) (4.9.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from sqlalchemy) (3.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00c0c0d7-f7ea-4bac-b7a1-cd7ef7130508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\usuario\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a25330a-7efd-43aa-98d5-f339631a0d37",
   "metadata": {},
   "source": [
    "pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7ae2ca-64d1-4995-8ec8-b4c761905334",
   "metadata": {},
   "source": [
    "### Para conectar y trabajar con PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad865b5c-6bd2-4b71-930c-511531905aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.10-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Downloading psycopg2_binary-2.9.10-cp311-cp311-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 7.2 MB/s eta 0:00:00\n",
      "Installing collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.10\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c167ef-9621-46ea-be56-d67ac5b67026",
   "metadata": {},
   "source": [
    "#### Aclaración\n",
    "psycopg2-binary es la versión precompilada de psycopg2, que facilita su instalación en diferentes entornos."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2f05d63-b805-4fde-9585-a46fbaff962e",
   "metadata": {},
   "source": [
    "A continuación dejo en formato RAW los pasos de conexión a PostgreSQL para explorarlos\n",
    "Configura la Conexión a PostgreSQL\n",
    "Para conectarte a una base de datos PostgreSQL, necesitas una cadena de conexión con el siguiente formato:\n",
    "postgresql://usuario:contraseña@localhost:puerto/nombre_bd\n",
    "\n",
    "\n",
    "usuario: tu nombre de usuario en PostgreSQL.\n",
    "contraseña: la contraseña de tu usuario.\n",
    "localhost: la dirección del servidor de la base de datos (puede cambiar si estás en un servidor remoto).\n",
    "puerto: el puerto en el que PostgreSQL está escuchando (por defecto es 5432).\n",
    "nombre_bd: el nombre de la base de datos a la que quieres conectarte.\n",
    "\n",
    " Conéctate a la Base de Datos\n",
    "Utiliza SQLAlchemy para crear el motor de conexión (engine) a la base de datos. Aquí tienes un ejemplo:\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Configura la cadena de conexión\n",
    "database_url = \"postgresql://usuario:contraseña@localhost:5432/nombre_bd\"\n",
    "\n",
    "# Crea el motor de conexión\n",
    "engine = create_engine(database_url)\n",
    "\n",
    "Asegúrate de reemplazar \"postgresql://usuario:contraseña@localhost:5432/nombre_bd\" con los valores correctos para tu base de datos.\n",
    "\n",
    "Realiza Consultas SQL\n",
    "Puedes utilizar pandas para realizar consultas SQL y cargar los datos en un DataFrame. Esto es especialmente útil para análisis de datos en Jupyter Notebook.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Ejecuta una consulta SQL y carga los resultados en un DataFrame\n",
    "query = \"SELECT * FROM nombre_tabla;\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Muestra los primeros registros\n",
    "df.head()\n",
    "\n",
    "En este ejemplo, \"SELECT * FROM nombre_tabla;\" es una consulta SQL que selecciona todos los datos de nombre_tabla. Cambia \"nombre_tabla\" por el nombre real de la tabla que deseas consultar.\n",
    "\n",
    "Inserta Datos en la Base de Datos (opcional)\n",
    "Si necesitas insertar datos en PostgreSQL desde un DataFrame, puedes usar el método to_sql de pandas:\n",
    "# Supón que 'df' es un DataFrame que quieres cargar en PostgreSQL\n",
    "df.to_sql('nombre_tabla', engine, if_exists='append', index=False)\n",
    "\n",
    "'nombre_tabla': es el nombre de la tabla en la base de datos donde quieres insertar los datos.\n",
    "if_exists='append': añade los datos al final de la tabla. Usa 'replace' si quieres reemplazar la tabla existente.\n",
    "index=False: evita que pandas cree una columna adicional para los índices del DataFrame.\n",
    "\n",
    "Cierra la Conexión (opcional)\n",
    "Al finalizar, puedes cerrar la conexión del motor si no planeas hacer más consultas:\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a97412d-b9c2-4a8d-b931-ca9952fce292",
   "metadata": {},
   "source": [
    "### Ejemplo Completo\n",
    "Aquí tienes un ejemplo completo de cómo conectarte, realizar una consulta y mostrar los datos en Jupyter Notebook:\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Conectar a PostgreSQL\n",
    "database_url = \"postgresql://usuario:contraseña@localhost:5432/nombre_bd\"\n",
    "engine = create_engine(database_url)\n",
    "\n",
    "# Realizar una consulta SQL\n",
    "query = \"SELECT * FROM nombre_tabla;\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Mostrar los datos\n",
    "print(df.head())\n",
    "\n",
    "# Cerrar la conexión\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb04bf5-572a-4b3b-85f7-0ef56f628ab9",
   "metadata": {},
   "source": [
    "## Paso 2: Define el Proceso ETL\n",
    "El proceso ETL consta de tres pasos principales: extracción, transformación y carga. A continuación se detalla cómo implementar cada uno.\n",
    "\n",
    "## Paso 3: Extracción (Extract)\n",
    "En esta etapa, el objetivo es obtener datos desde una API de datos inmobiliarios (o una fuente externa de tu elección). Usaremos la biblioteca requests para hacer la solicitud HTTP y obtener los datos en formato JSON."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0bc33caa-d74b-4316-ae59-f10cd0d15fe4",
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "def extract_data_from_api(api_url):\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()  # Asumiendo que la respuesta es JSON\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Error al obtener los datos: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Importante reemplazar api_url con la url real de la API una vez se haya decidido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bef37cf-9970-4595-921c-996542746af3",
   "metadata": {},
   "source": [
    "## Paso 4: Transformación (Transform)\n",
    "La etapa de transformación implica limpiar y normalizar los datos para que coincidan con el formato de la base de datos de REFFINDR.\n",
    "\n",
    "Para ello debo convertir los datos en un DataFrame de pandas.\n",
    "Realiza las transformaciones necesarias (filtrado, cambio de nombres de columnas, etc.)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d869d8d7-576e-4434-8649-3acfc5296b9b",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "def transform_data(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ejemplo de transformación: renombrar columnas\n",
    "    df = df.rename(columns={\n",
    "        \"price\": \"precio\",\n",
    "        \"location\": \"ubicacion\",\n",
    "        \"type\": \"tipo_propiedad\",\n",
    "        # Añade otros cambios según sea necesario\n",
    "    })\n",
    "    \n",
    "    # Filtros adicionales y normalización\n",
    "    df = df[df['precio'].notnull()]  # Filtra propiedades con precio disponible\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07dd398-e260-4118-b6cf-f4e6a5179e65",
   "metadata": {},
   "source": [
    "El código anterior se debe seguir explorando en base a las especificaciones del dataframe elegido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d671bc-8875-4c5e-bdcd-a373612ee129",
   "metadata": {},
   "source": [
    "## Paso 5: Carga (Load)\n",
    "Para la carga en PostgreSQL, usare SQLAlchemy para la conexión. El código a continuación asume que ya tienes una base de datos en PostgreSQL configurada y lista para recibir los datos. \n",
    "### En caso de no ser así, modifica esa parte del código\n",
    "\n",
    "Crea la conexión con PostgreSQL.\n",
    "Carga los datos en la tabla correspondiente."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e466a7d-ec80-4466-9080-a9369432b20f",
   "metadata": {},
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "def load_data_to_db(df, database_url):\n",
    "    engine = create_engine(database_url)\n",
    "    \n",
    "    # Carga los datos en la tabla 'propiedades' de la base de datos\n",
    "    df.to_sql('propiedades', engine, if_exists='append', index=False)\n",
    "    print(\"Datos cargados exitosamente en la base de datos\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5645738-d1ab-4e8b-95e7-685100dbacb9",
   "metadata": {},
   "source": [
    "En database_url, coloca la cadena de conexión de tu base de datos PostgreSQL: \n",
    "database_url = 'postgresql://usuario:contraseña@localhost:5432/reffindr_db'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d5675-91fe-46f7-87ff-b96dc43820bd",
   "metadata": {},
   "source": [
    "En este paso debo asegurar de que  la tabla propiedades en PostgreSQL tiene una estructura compatible con el DataFrame que estás cargando."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492556f2-12c0-4795-a1a3-97890cd7cc7d",
   "metadata": {},
   "source": [
    "## Paso 6: Ejecuta el Proceso ETL Completo\n",
    "Ahora que tengo cada etapa definida, debo unificarlas en una función que ejecute todo el proceso."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a7237b8-b861-4d83-9366-3d2607d312bf",
   "metadata": {},
   "source": [
    "def run_etl(api_url, database_url):\n",
    "    # Extracción\n",
    "    data = extract_data_from_api(api_url)\n",
    "    if data is None:\n",
    "        print(\"Error: No se pudo extraer la información.\")\n",
    "        return\n",
    "    \n",
    "    # Transformación\n",
    "    df = transform_data(data)\n",
    "    \n",
    "    # Carga\n",
    "    load_data_to_db(df, database_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5466ccd8-ae68-449b-ba9c-123dc2c94203",
   "metadata": {},
   "source": [
    "## Paso 7: Configura y ejecuta el proceso\n",
    "Define la URL de la API y la conexión a la base de datos.\n",
    "Ejecuta el proceso ETL."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec52d71e-7409-4e37-a059-230a9ecd09aa",
   "metadata": {},
   "source": [
    "api_url = 'URL_DE_LA_API_INMOBILIARIA'\n",
    "database_url = 'postgresql://usuario:contraseña@localhost:5432/reffindr_db'\n",
    "\n",
    "# Ejecuta el proceso ETL\n",
    "run_etl(api_url, database_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e61a740-475d-4601-94b9-62960ea08583",
   "metadata": {},
   "source": [
    "Consideraciones Finales\n",
    "Automatización: Puedes automatizar este proceso programando su ejecución periódica, por ejemplo, usando un cron job en Linux o el programador de tareas en Windows.\n",
    "Validación de Datos: Asegúrate de validar los datos después de la carga, verificando que las estructuras y los tipos de datos se hayan insertado correctamente.\n",
    "Control de Errores: Añade manejo de errores para cada paso del proceso ETL. Esto es útil en caso de que la API no responda, o si hay problemas de conexión con la base de datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
